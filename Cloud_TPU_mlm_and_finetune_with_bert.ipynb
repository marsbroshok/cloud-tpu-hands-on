{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBcr1QSHtOw4"
   },
   "source": [
    "# End-to-end Masked Language Modeling with BERT\n",
    "\n",
    "**Author:** Ankur Singh, TPU adaptation Alexander Usoltsev<br>\n",
    "**Date created:** 2020/09/18<br>\n",
    "**Last modified:** 2020/09/18<br>\n",
    "**Description:** Implement a Masked Language Model (MLM) with BERT and fine-tune it on the IMDB Reviews dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SDIj0ZxtOw6"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Masked Language Modeling is a fill-in-the-blank task,\n",
    "where a model uses the context words surrounding a mask token to try to predict what the\n",
    "masked word should be.\n",
    "\n",
    "For an input that contains one or more mask tokens,\n",
    "the model will generate the most likely substitution for each.\n",
    "\n",
    "Example:\n",
    "\n",
    "- Input: \"I have watched this [MASK] and it was awesome.\"\n",
    "- Output: \"I have watched this movie and it was awesome.\"\n",
    "\n",
    "Masked language modeling is a great way to train a language\n",
    "model in a self-supervised setting (without human-annotated labels).\n",
    "Such a model can then be fine-tuned to accomplish various supervised\n",
    "NLP tasks.\n",
    "\n",
    "This example teaches you how to build a BERT model from scratch,\n",
    "train it with the masked language modeling task,\n",
    "and then fine-tune this model on a sentiment classification task.\n",
    "\n",
    "We will use the Keras `TextVectorization` and `MultiHeadAttention` layers\n",
    "to create a BERT Transformer-Encoder network architecture.\n",
    "\n",
    "Note: This example should be run with `tf-nightly`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88doWCD2tOw7"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Install `tf-nightly` via `pip install tf-nightly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zCq4SqV3tOw8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the `TPU_NAME` env variable below so the TPUResolver can find your TPU instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TPU_NAME=auv-cloud-tpu\n"
     ]
    }
   ],
   "source": [
    "%env TPU_NAME=auv-cloud-tpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdx12nLRtol8",
    "outputId": "241c0c26-1a11-4221-d88c-33ddf45f94f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: auv-cloud-tpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: auv-cloud-tpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU')]\n",
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "try:  # detect TPU\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()  # resolver will search for the TPU_NAME env var\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    # This is the TPU initialization code that has to be at the beginning.\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)\n",
    "except ValueError:  # detect GPUs\n",
    "    strategy = tf.distribute.MirroredStrategy()  # for GPU or multi-GPU machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQ2X1K4_tOw9"
   },
   "source": [
    "## Set-up Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6DI7rPhftOw9"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LEN = 256\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 0.001\n",
    "    VOCAB_SIZE = 30000\n",
    "    EMBED_DIM = 128\n",
    "    NUM_HEAD = 8  # used in bert model\n",
    "    FF_DIM = 128  # used in bert model\n",
    "    NUM_LAYERS = 1\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ul2dAkW1tOw_"
   },
   "source": [
    "## Load the data\n",
    "\n",
    "We will first download the movie reviews data and load into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-12 14:33:45--  https://github.com/TheophileBlard/french-sentiment-analysis-with-bert/raw/master/allocine_dataset/data.tar.bz2\n",
      "Resolving github.com (github.com)... 140.82.114.3\n",
      "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/TheophileBlard/french-sentiment-analysis-with-bert/master/allocine_dataset/data.tar.bz2 [following]\n",
      "--2021-07-12 14:33:45--  https://raw.githubusercontent.com/TheophileBlard/french-sentiment-analysis-with-bert/master/allocine_dataset/data.tar.bz2\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 66625305 (64M) [application/octet-stream]\n",
      "Saving to: ‘data.tar.bz2’\n",
      "\n",
      "data.tar.bz2        100%[===================>]  63.54M  82.8MB/s    in 0.8s    \n",
      "\n",
      "2021-07-12 14:33:46 (82.8 MB/s) - ‘data.tar.bz2’ saved [66625305/66625305]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Allocine dataset from https://github.com/TheophileBlard/french-sentiment-analysis-with-bert\n",
    "!wget https://github.com/TheophileBlard/french-sentiment-analysis-with-bert/raw/master/allocine_dataset/data.tar.bz2\n",
    "!tar -xf data.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(fn):\n",
    "    df = pd.read_json(fn, lines=True)\n",
    "    df['sentiment'] = df['polarity']\n",
    "    return df[['review', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = get_data('data/train.jsonl').sample(n=35000)  # For demo let's reduce the dataset size\n",
    "test_df = get_data('data/test.jsonl').sample(n=10000)\n",
    "all_data = train_df.append(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSgL1a4Mu9_K",
    "outputId": "d9aaa579-6194-4b84-8adf-54a9f8e555c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 45000 entries, 98322 to 1850\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     45000 non-null  object\n",
      " 1   sentiment  45000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "all_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98322</th>\n",
       "      <td>Ce blockbuster signé Disney &amp; Jerry Bruckheime...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81022</th>\n",
       "      <td>Je l'ai vu en Imax à la Géode, ce qui m'a perm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>Je me suis terriblement ennuyé. Vu les autres ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86257</th>\n",
       "      <td>Mais que reproche-t-on au juste à ce film ? Le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40322</th>\n",
       "      <td>Ce film c toute mon enfance ! Le 1 aussi mais ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "98322  Ce blockbuster signé Disney & Jerry Bruckheime...          0\n",
       "81022  Je l'ai vu en Imax à la Géode, ce qui m'a perm...          1\n",
       "676    Je me suis terriblement ennuyé. Vu les autres ...          0\n",
       "86257  Mais que reproche-t-on au juste à ce film ? Le...          1\n",
       "40322  Ce film c toute mon enfance ! Le 1 aussi mais ...          1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJy4Ptz_tOxB"
   },
   "source": [
    "## Dataset preparation\n",
    "\n",
    "We will use the `TextVectorization` layer to vectorize the text into integer token ids.\n",
    "It transforms a batch of strings into either\n",
    "a sequence of token indices (one sample = 1D array of integer token indices, in order)\n",
    "or a dense representation (one sample = 1D array of float values encoding an unordered set of tokens).\n",
    "\n",
    "Below, we define 3 preprocessing functions.\n",
    "\n",
    "1.  The `get_vectorize_layer` function builds the `TextVectorization` layer.\n",
    "2.  The `encode` function encodes raw text into integer token ids.\n",
    "3.  The `get_masked_input_and_labels` function will mask input token ids.\n",
    "It masks 15% of all input tokens in each sequence at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0KNfffDVtOxB"
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(\"!#$%&()*+,-./:;<=>?@\\^_`{|}~\"), \"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=[\"[MASK]\"]):\n",
    "    \"\"\"Build Text vectorization layer\n",
    "\n",
    "    Args:\n",
    "      texts (list): List of string i.e input texts\n",
    "      vocab_size (int): vocab size\n",
    "      max_seq (int): Maximum sequence lenght.\n",
    "      special_tokens (list, optional): List of special tokens. Defaults to ['[MASK]'].\n",
    "\n",
    "    Returns:\n",
    "        layers.Layer: Return TextVectorization Keras Layer\n",
    "    \"\"\"\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode=\"int\",\n",
    "        standardize=custom_standardization,\n",
    "        output_sequence_length=max_seq,\n",
    "    )\n",
    "    vectorize_layer.adapt(texts)\n",
    "\n",
    "    # Insert mask token in vocabulary\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    vocab = vocab[2 : vocab_size - len(special_tokens)] + [\"[mask]\"]\n",
    "    vectorize_layer.set_vocabulary(vocab)\n",
    "    return vectorize_layer\n",
    "\n",
    "def encode(texts):\n",
    "    encoded_texts = vectorize_layer(texts)\n",
    "    return encoded_texts.numpy()\n",
    "\n",
    "\n",
    "def get_masked_input_and_labels(encoded_texts):\n",
    "    # 15% BERT masking\n",
    "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
    "    # Do not mask special tokens\n",
    "    inp_mask[encoded_texts <= 2] = False\n",
    "    # Set targets to -1 by default, it means ignore\n",
    "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
    "    # Set labels for masked tokens\n",
    "    labels[inp_mask] = encoded_texts[inp_mask]\n",
    "\n",
    "    # Prepare input\n",
    "    encoded_texts_masked = np.copy(encoded_texts)\n",
    "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
    "    # This means leaving 10% unchanged\n",
    "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
    "    encoded_texts_masked[\n",
    "        inp_mask_2mask\n",
    "    ] = mask_token_id  # mask token is the last in the dict\n",
    "\n",
    "    # Set 10% to a random token\n",
    "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
    "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
    "        3, mask_token_id, inp_mask_2random.sum()\n",
    "    )\n",
    "\n",
    "    # Prepare sample_weights to pass to .fit() method\n",
    "    sample_weights = np.ones(labels.shape)\n",
    "    sample_weights[labels == -1] = 0\n",
    "\n",
    "    # y_labels would be same as encoded_texts i.e input tokens\n",
    "    y_labels = np.copy(encoded_texts)\n",
    "\n",
    "    return encoded_texts_masked, y_labels, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 3s, sys: 45.1 s, total: 2min 48s\n",
      "Wall time: 4min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorize_layer = get_vectorize_layer(\n",
    "    all_data.review.values.tolist(),\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    "    special_tokens=[\"[mask]\"],\n",
    ")\n",
    "\n",
    "# Get mask token id for masked language model\n",
    "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.05 s, sys: 829 ms, total: 1.88 s\n",
      "Wall time: 4.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Examples for training\n",
    "x_train = encode(train_df.review.values)  # encode reviews with vectorizer\n",
    "y_train = train_df.sentiment.values\n",
    "train_classifier_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    .shuffle(1000)\n",
    "    .batch(config.BATCH_SIZE)\n",
    ")\n",
    "\n",
    "# We have 25000 examples for testing\n",
    "x_test = encode(test_df.review.values)\n",
    "y_test = test_df.sentiment.values\n",
    "test_classifier_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(\n",
    "    config.BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Build dataset for end to end model input (will be used at the end)\n",
    "test_raw_classifier_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (test_df.review.values, y_test)\n",
    ").batch(config.BATCH_SIZE)\n",
    "\n",
    "# Prepare data for masked language model\n",
    "x_all_review = encode(all_data.review.values)\n",
    "x_masked_train, y_masked_labels, sample_weights = get_masked_input_and_labels(\n",
    "    x_all_review\n",
    ")\n",
    "\n",
    "mlm_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_masked_train, y_masked_labels, sample_weights)\n",
    ")\n",
    "mlm_ds = mlm_ds.shuffle(1000).batch(config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjPTtAKWtOxC"
   },
   "source": [
    "## Create BERT model (Pretraining Model) for masked language modeling\n",
    "\n",
    "We will create a BERT-like pretraining model architecture\n",
    "using the `MultiHeadAttention` layer.\n",
    "It will take token ids as inputs (including masked tokens)\n",
    "and it will predict the correct ids for the masked input tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "oCSVfnWDtOxC"
   },
   "outputs": [],
   "source": [
    "def bert_module(query, key, value, i):\n",
    "    # Multi headed self-attention\n",
    "    attention_output = layers.MultiHeadAttention(\n",
    "        num_heads=config.NUM_HEAD,\n",
    "        key_dim=config.EMBED_DIM // config.NUM_HEAD,\n",
    "        name=\"encoder_{}/multiheadattention\".format(i),\n",
    "    )(query, key, value)\n",
    "    attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n",
    "        attention_output\n",
    "    )\n",
    "    attention_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n",
    "    )(query + attention_output)\n",
    "\n",
    "    # Feed-forward layer\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(config.FF_DIM, activation=\"relu\"),\n",
    "            layers.Dense(config.EMBED_DIM),\n",
    "        ],\n",
    "        name=\"encoder_{}/ffn\".format(i),\n",
    "    )\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(\n",
    "        ffn_output\n",
    "    )\n",
    "    sequence_output = layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n",
    "    )(attention_output + ffn_output)\n",
    "    return sequence_output\n",
    "\n",
    "\n",
    "def get_pos_encoding_matrix(max_len, d_emb):\n",
    "    pos_enc = np.array(\n",
    "        [\n",
    "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
    "            if pos != 0\n",
    "            else np.zeros(d_emb)\n",
    "            for pos in range(max_len)\n",
    "        ]\n",
    "    )\n",
    "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
    "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
    "    return pos_enc\n",
    "\n",
    "class MaskedLanguageModel(tf.keras.Model):\n",
    "    def train_step(self, inputs):\n",
    "        if len(inputs) == 3:\n",
    "            features, labels, sample_weight = inputs\n",
    "        else:\n",
    "            features, labels = inputs\n",
    "            sample_weight = None\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(features, training=True)\n",
    "            loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        loss_tracker.update_state(loss, sample_weight=sample_weight)\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker]\n",
    "\n",
    "\n",
    "def create_masked_language_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "\n",
    "    word_embeddings = layers.Embedding(\n",
    "        config.VOCAB_SIZE, config.EMBED_DIM, name=\"word_embedding\"\n",
    "    )(inputs)\n",
    "    position_embeddings = layers.Embedding(\n",
    "        input_dim=config.MAX_LEN,\n",
    "        output_dim=config.EMBED_DIM,\n",
    "        weights=[get_pos_encoding_matrix(config.MAX_LEN, config.EMBED_DIM)],\n",
    "        name=\"position_embedding\",\n",
    "    )(tf.range(start=0, limit=config.MAX_LEN, delta=1))\n",
    "    embeddings = word_embeddings + position_embeddings\n",
    "\n",
    "    encoder_output = embeddings\n",
    "    for i in range(config.NUM_LAYERS):\n",
    "        encoder_output = bert_module(encoder_output, encoder_output, encoder_output, i)\n",
    "\n",
    "    mlm_output = layers.Dense(config.VOCAB_SIZE, name=\"mlm_cls\", activation=\"softmax\")(\n",
    "        encoder_output\n",
    "    )\n",
    "    mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    mlm_model.compile(optimizer=optimizer)\n",
    "    return mlm_model\n",
    "\n",
    "\n",
    "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
    "token2id = {y: x for x, y in id2token.items()}\n",
    "\n",
    "\n",
    "class MaskedTextGenerator(keras.callbacks.Callback):\n",
    "    def __init__(self, sample_tokens, top_k=5):\n",
    "        self.sample_tokens = sample_tokens\n",
    "        self.k = top_k\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \" \".join([id2token[t] for t in tokens if t != 0])\n",
    "\n",
    "    def convert_ids_to_tokens(self, id):\n",
    "        return id2token[id]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        prediction = self.model.predict(self.sample_tokens)\n",
    "\n",
    "        masked_index = np.where(self.sample_tokens == mask_token_id)\n",
    "        masked_index = masked_index[1]\n",
    "        mask_prediction = prediction[0][masked_index]\n",
    "\n",
    "        top_indices = mask_prediction[0].argsort()[-self.k :][::-1]\n",
    "        values = mask_prediction[0][top_indices]\n",
    "\n",
    "        for i in range(len(top_indices)):\n",
    "            p = top_indices[i]\n",
    "            v = values[i]\n",
    "            tokens = np.copy(sample_tokens[0])\n",
    "            tokens[masked_index[0]] = p\n",
    "            result = {\n",
    "                \"input_text\": self.decode(sample_tokens[0].numpy()),\n",
    "                \"prediction\": self.decode(tokens),\n",
    "                \"probability\": v,\n",
    "                \"predicted mask token\": self.convert_ids_to_tokens(p),\n",
    "            }\n",
    "            pprint(result)\n",
    "\n",
    "\n",
    "# sample_tokens = vectorize_layer([\"I have watched this  and it was awesome\"])\n",
    "sample_tokens = vectorize_layer([\"J'ai [mask] ce film et c'était genial\"])\n",
    "generator_callback = MaskedTextGenerator(sample_tokens.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z7qyb6t-ujZg",
    "outputId": "6833bf73-a753-40e7-c682-1cacac2ea317"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"masked_bert_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 256, 128)     3840000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 256, 128)     0           word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/multiheadattention (M (None, 256, 128)     66048       tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "                                                                 tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/att_dropout (Dropout) (None, 256, 128)     0           encoder_0/multiheadattention[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 256, 128)     0           tf.__operators__.add[0][0]       \n",
      "                                                                 encoder_0/att_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/att_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn (Sequential)      (None, 256, 128)     33024       encoder_0/att_layernormalization[\n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn_dropout (Dropout) (None, 256, 128)     0           encoder_0/ffn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 256, 128)     0           encoder_0/att_layernormalization[\n",
      "                                                                 encoder_0/ffn_dropout[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "encoder_0/ffn_layernormalizatio (None, 256, 128)     256         tf.__operators__.add_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mlm_cls (Dense)                 (None, 256, 30000)   3870000     encoder_0/ffn_layernormalization[\n",
      "==================================================================================================\n",
      "Total params: 7,809,584\n",
      "Trainable params: 7,809,584\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
    "        reduction=tf.keras.losses.Reduction.NONE\n",
    "    )\n",
    "    loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n",
    "    bert_masked_model = create_masked_language_bert_model()\n",
    "    \n",
    "bert_masked_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5r7D9fgtOxF"
   },
   "source": [
    "## Train and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2D3OSJuXtOxH",
    "outputId": "0ed29c9b-6dc4-4b20-f7d2-d13e5ce649d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   4/1407 [..............................] - ETA: 23s - loss: 10.2260    WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 1.0250s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0017s vs `on_train_batch_end` time: 1.0250s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 37s 19ms/step - loss: 7.1940\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'film',\n",
      " 'prediction': \"j'ai film ce film et c'était genial\",\n",
      " 'probability': 0.14764087}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'très',\n",
      " 'prediction': \"j'ai très ce film et c'était genial\",\n",
      " 'probability': 0.036560558}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'bon',\n",
      " 'prediction': \"j'ai bon ce film et c'était genial\",\n",
      " 'probability': 0.032476593}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'je',\n",
      " 'prediction': \"j'ai je ce film et c'était genial\",\n",
      " 'probability': 0.03156673}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'un',\n",
      " 'prediction': \"j'ai un ce film et c'était genial\",\n",
      " 'probability': 0.030473072}\n",
      "Epoch 2/5\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 6.7242\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'film',\n",
      " 'prediction': \"j'ai film ce film et c'était genial\",\n",
      " 'probability': 0.3162167}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'bon',\n",
      " 'prediction': \"j'ai bon ce film et c'était genial\",\n",
      " 'probability': 0.15228908}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'suis',\n",
      " 'prediction': \"j'ai suis ce film et c'était genial\",\n",
      " 'probability': 0.028014654}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'scénario',\n",
      " 'prediction': \"j'ai scénario ce film et c'était genial\",\n",
      " 'probability': 0.015533536}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'beau',\n",
      " 'prediction': \"j'ai beau ce film et c'était genial\",\n",
      " 'probability': 0.01502686}\n",
      "Epoch 3/5\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 6.0872\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'bon',\n",
      " 'prediction': \"j'ai bon ce film et c'était genial\",\n",
      " 'probability': 0.24585629}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'très',\n",
      " 'prediction': \"j'ai très ce film et c'était genial\",\n",
      " 'probability': 0.16759002}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'film',\n",
      " 'prediction': \"j'ai film ce film et c'était genial\",\n",
      " 'probability': 0.10390213}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'super',\n",
      " 'prediction': \"j'ai super ce film et c'était genial\",\n",
      " 'probability': 0.04289582}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'beau',\n",
      " 'prediction': \"j'ai beau ce film et c'était genial\",\n",
      " 'probability': 0.04246954}\n",
      "Epoch 4/5\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 5.2109\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'adoré',\n",
      " 'prediction': \"j'ai adoré ce film et c'était genial\",\n",
      " 'probability': 0.07757501}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'voir',\n",
      " 'prediction': \"j'ai voir ce film et c'était genial\",\n",
      " 'probability': 0.05301281}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'que',\n",
      " 'prediction': \"j'ai que ce film et c'était genial\",\n",
      " 'probability': 0.044784445}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'film',\n",
      " 'prediction': \"j'ai film ce film et c'était genial\",\n",
      " 'probability': 0.039161656}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'vu',\n",
      " 'prediction': \"j'ai vu ce film et c'était genial\",\n",
      " 'probability': 0.03751531}\n",
      "Epoch 5/5\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 4.5853\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'adoré',\n",
      " 'prediction': \"j'ai adoré ce film et c'était genial\",\n",
      " 'probability': 0.21366425}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'vu',\n",
      " 'prediction': \"j'ai vu ce film et c'était genial\",\n",
      " 'probability': 0.10837262}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'lu',\n",
      " 'prediction': \"j'ai lu ce film et c'était genial\",\n",
      " 'probability': 0.059990168}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'découvert',\n",
      " 'prediction': \"j'ai découvert ce film et c'était genial\",\n",
      " 'probability': 0.048507873}\n",
      "{'input_text': \"j'ai [mask] ce film et c'était genial\",\n",
      " 'predicted mask token': 'trouvé',\n",
      " 'prediction': \"j'ai trouvé ce film et c'était genial\",\n",
      " 'probability': 0.044816747}\n"
     ]
    }
   ],
   "source": [
    "bert_masked_model.fit(mlm_ds, epochs=5, callbacks=[generator_callback])\n",
    "bert_masked_model.save(\"/tmp/bert_mlm_allocine.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbLgk_BetOxH"
   },
   "source": [
    "## Fine-tune a sentiment classification model\n",
    "\n",
    "We will fine-tune our self-supervised model on a downstream task of sentiment classification.\n",
    "To do this, let's create a classifier by adding a pooling layer and a `Dense` layer on top of the\n",
    "pretrained BERT features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vbvvZ5g2tOxI",
    "outputId": "d9680875-12d2-4095-d5d6-c586ad17d02d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 256)]             0         \n",
      "_________________________________________________________________\n",
      "model_1 (Functional)         (None, 256, 128)          3939584   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,947,905\n",
      "Trainable params: 8,321\n",
      "Non-trainable params: 3,939,584\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1094/1094 [==============================] - 19s 14ms/step - loss: 0.6996 - accuracy: 0.5952 - val_loss: 0.6024 - val_accuracy: 0.6783\n",
      "Epoch 2/5\n",
      "1094/1094 [==============================] - 12s 11ms/step - loss: 0.6279 - accuracy: 0.6496 - val_loss: 0.6374 - val_accuracy: 0.6521\n",
      "Epoch 3/5\n",
      "1094/1094 [==============================] - 13s 12ms/step - loss: 0.6130 - accuracy: 0.6673 - val_loss: 0.5938 - val_accuracy: 0.6875\n",
      "Epoch 4/5\n",
      "1094/1094 [==============================] - 13s 12ms/step - loss: 0.6050 - accuracy: 0.6743 - val_loss: 0.5793 - val_accuracy: 0.6959\n",
      "Epoch 5/5\n",
      "1094/1094 [==============================] - 13s 11ms/step - loss: 0.6049 - accuracy: 0.6699 - val_loss: 0.5788 - val_accuracy: 0.6919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f20b3c76150>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_classifier_bert_model():\n",
    "    inputs = layers.Input((config.MAX_LEN,), dtype=tf.int64)\n",
    "    sequence_output = pretrained_bert_model(inputs)\n",
    "    pooled_output = layers.GlobalMaxPooling1D()(sequence_output)\n",
    "    hidden_layer = layers.Dense(64, activation=\"relu\")(pooled_output)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "    classifer_model = keras.Model(inputs, outputs, name=\"classification\")\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    classifer_model.compile(\n",
    "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return classifer_model\n",
    "\n",
    "with strategy.scope():\n",
    "    # Load pretrained bert model\n",
    "    mlm_model = keras.models.load_model(\n",
    "        \"/tmp/bert_mlm_allocine.h5\", custom_objects={\"MaskedLanguageModel\": MaskedLanguageModel}\n",
    "    )\n",
    "    pretrained_bert_model = tf.keras.Model(\n",
    "        mlm_model.input, mlm_model.get_layer(\"encoder_0/ffn_layernormalization\").output\n",
    "    )\n",
    "\n",
    "    # Freeze it\n",
    "    pretrained_bert_model.trainable = False\n",
    "    classifer_model = create_classifier_bert_model()\n",
    "\n",
    "classifer_model.summary()\n",
    "\n",
    "# Train the classifier with frozen BERT stage\n",
    "classifer_model.fit(\n",
    "    train_classifier_ds,\n",
    "    epochs=5,\n",
    "    validation_data=test_classifier_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1094/1094 [==============================] - 25s 17ms/step - loss: 0.3654 - accuracy: 0.8316 - val_loss: 0.2535 - val_accuracy: 0.8929\n",
      "Epoch 2/5\n",
      "1094/1094 [==============================] - 15s 13ms/step - loss: 0.1939 - accuracy: 0.9211 - val_loss: 0.2843 - val_accuracy: 0.8882\n",
      "Epoch 3/5\n",
      "1094/1094 [==============================] - 14s 13ms/step - loss: 0.1019 - accuracy: 0.9629 - val_loss: 0.2861 - val_accuracy: 0.9033\n",
      "Epoch 4/5\n",
      "1094/1094 [==============================] - 14s 13ms/step - loss: 0.0491 - accuracy: 0.9837 - val_loss: 0.4105 - val_accuracy: 0.8941\n",
      "Epoch 5/5\n",
      "1094/1094 [==============================] - 14s 13ms/step - loss: 0.0358 - accuracy: 0.9873 - val_loss: 0.4342 - val_accuracy: 0.8982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f20b70ffc10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unfreeze the BERT model for fine-tuning\n",
    "with strategy.scope():\n",
    "    pretrained_bert_model.trainable = True\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    classifer_model.compile(\n",
    "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "classifer_model.fit(\n",
    "    train_classifier_ds,\n",
    "    epochs=5,\n",
    "    validation_data=test_classifier_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjkLfmLutOxI"
   },
   "source": [
    "## Create an end-to-end model and evaluate it\n",
    "\n",
    "When you want to deploy a model, it's best if it already includes its preprocessing\n",
    "pipeline, so that you don't have to reimplement the preprocessing logic in your\n",
    "production environment. Let's create an end-to-end model that incorporates\n",
    "the `TextVectorization` layer, and let's evaluate. Our model will accept raw strings\n",
    "as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N7r4iOyotOxI",
    "outputId": "e92c69e6-6b76-4a71-85d4-bde972ccf344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 21s 63ms/step - loss: 0.4345 - accuracy: 0.8980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4344607889652252, 0.8980000019073486]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_end_to_end(model):\n",
    "    inputs_string = keras.Input(shape=(1,), dtype=\"string\")\n",
    "    indices = vectorize_layer(inputs_string)\n",
    "    outputs = model(indices)\n",
    "    end_to_end_model = keras.Model(inputs_string, outputs, name=\"end_to_end_model\")\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=config.LR)\n",
    "    end_to_end_model.compile(\n",
    "        optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return end_to_end_model\n",
    "\n",
    "\n",
    "end_to_end_classification_model = get_end_to_end(classifer_model)\n",
    "end_to_end_classification_model.evaluate(test_raw_classifier_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqdPnO6n0KPN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "cloud_tpu_mlm_and_finetune_with_bert",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
