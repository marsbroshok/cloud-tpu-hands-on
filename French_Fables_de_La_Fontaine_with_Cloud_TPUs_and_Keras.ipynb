{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6ZDpd9XzFeN"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Hub Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "KUu4vOt5zI9d"
   },
   "outputs": [],
   "source": [
    "# Copyright 2018 The TensorFlow Hub Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edfbxDDh2AEs"
   },
   "source": [
    "## Predict Fables de La Fontaine with Cloud TPUs and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNo1Vfghpa8j"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This example uses [tf.keras](https://www.tensorflow.org/guide/keras) to build a *language model* and train it on a Cloud TPU. This language model predicts the next character of text given the text so far. The trained model can generate new snippets of text that read in a similar style to the text training data.\n",
    "\n",
    "The model trains for 10 epochs and completes in approximately 5 minutes.\n",
    "\n",
    "This notebook is hosted on GitHub. To view it in its original repository, after opening the notebook, select **File > View on GitHub**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgAHfQtuhddd"
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "In this Colab, you will learn how to:\n",
    "*   Build a two-layer, forward-LSTM model.\n",
    "*   Use distribution strategy to produce a `tf.keras` model that runs on TPU version and then use the standard Keras methods to train: `fit`, `predict`, and `evaluate`.\n",
    "*   Use the trained model to make predictions and generate your own La Fontaine-esque play.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrprJD-R-410"
   },
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_I0RdnOSkNmi"
   },
   "source": [
    "<h3>  &nbsp;&nbsp;Train on TPU&nbsp;&nbsp; <a href=\"https://cloud.google.com/tpu/\"><img valign=\"middle\" src=\"https://raw.githubusercontent.com/GoogleCloudPlatform/tensorflow-without-a-phd/master/tensorflow-rl-pong/images/tpu-hexagon.png\" width=\"50\"></a></h3>\n",
    "\n",
    "If you are using Google Colab:\n",
    "   1. On the main menu, click Runtime and select **Change runtime type**. Set \"TPU\" as the hardware accelerator.\n",
    "   1. Click Runtime again and select **Runtime > Run All**. You can also run the cells manually with Shift-ENTER. \n",
    "   \n",
    "If you are using **Google Cloud Platform**:\n",
    "\n",
    "   1. Follow the [Cloud TPU](https://cloud.google.com/tpu/docs/creating-deleting-tpus) documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYxeFuKCUx9d"
   },
   "source": [
    "TPUs are located in Google Cloud, for optimal performance, they read data directly from Google Cloud Storage (GCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lvo0t7XVIkWZ"
   },
   "source": [
    "## Data, model, and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzpUtDMqmA-x"
   },
   "source": [
    "In this example, you train the model on the combined works of Jean de La Fontaine, then use the model to compose a \"fable\" in his style:\n",
    "\n",
    "<blockquote>\n",
    "Maître corbeau, sur un arbre perché,\n",
    "          Tenoit en son bec un fromage.\n",
    "      Maître renard, par l'odeur alléché,\n",
    "          Lui tint à peu près ce langage:\n",
    "          Hé bonjour, monsieur du _  \n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRQ6Fjra3Ruq"
   },
   "source": [
    "### Download data\n",
    "\n",
    "Download a single text file from [Project Gutenberg](https://www.gutenberg.org/). You use snippets from this file as the *training data* for the model. The *target* snippet is offset by one character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8sIXh1DEDDd",
    "outputId": "8c714be1-a546-495c-e1dd-dd279e9b66ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-12 12:51:14--  https://www.gutenberg.org/files/56327/56327-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 416 Requested Range Not Satisfiable\n",
      "\n",
      "    The file is already fully retrieved; nothing to do.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget --show-progress --continue -O ./fontaine.txt https://www.gutenberg.org/files/56327/56327-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbL6cqCl7hnt"
   },
   "source": [
    "### Build the input dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7nbGKAHi0dx"
   },
   "source": [
    "We just downloaded some text. The following shows the start of the text and a random snippet so we can get a feel for the whole text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJiYai-GjQRk",
    "outputId": "6f1dd8ae-7af5-4e14-fb67-76da69d13264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of Fables, by Jean de  la Fontaine\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and most\n",
      "other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever.  You may copy it, give it away or re-use it under the terms of\n",
      "...\n",
      "          Bientôt à sec; et ce sera\n",
      "  Mais sûres cependant, et quelquefois cruelles.\n",
      "vit, chacun s'éclata de rire: personne ne s'imagina qu'il pût rien\n",
      "  Et le lard qui périt en cette occasion!\n",
      "not protected by U.S. copyright law. Redistribution is subject to the\n"
     ]
    }
   ],
   "source": [
    "!head -n5 ./fontaine.txt\n",
    "!echo \"...\"\n",
    "!shuf -n5 ./fontaine.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "E3V4V-Jxmuv3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "import distutils\n",
    "if distutils.version.LooseVersion(tf.__version__) < '2.0':\n",
    "    raise Exception('This notebook is compatible with TensorFlow 2.0 or higher.')\n",
    "\n",
    "INPUT_TXT = './fontaine.txt'\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "def transform(txt):\n",
    "  return np.asarray([ord(c) for c in txt if ord(c) < 255], dtype=np.int32)\n",
    "\n",
    "def input_fn(seq_len=100, batch_size=BATCH_SIZE):\n",
    "  \"\"\"Return a dataset of source and target sequences for training.\"\"\"\n",
    "  with tf.io.gfile.GFile(INPUT_TXT, 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "  source = tf.constant(transform(txt), dtype=tf.int32)\n",
    "\n",
    "  ds = tf.data.Dataset.from_tensor_slices(source).batch(seq_len+1, drop_remainder=True)\n",
    "\n",
    "  def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "  BUFFER_SIZE = 10000\n",
    "  ds = ds.map(split_input_target).shuffle(BUFFER_SIZE).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "  return ds.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K-Cgtg8wviRZ"
   },
   "outputs": [],
   "source": [
    "in_chars, out_chars = input_fn().as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "erIbI3JIv0uf",
    "outputId": "bb56533c-a610-47fd-a2a4-77a2d8c3cc48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1024, 100), (1024, 100))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_chars.shape, out_chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7411WM-wDw5",
    "outputId": "161ff62a-cfac-42d4-80e1-fe3eb06519c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 32, 110,  39, ...,  32,  32,  67],\n",
       "       [ 69, 120, 101, ...,  65,  99, 104],\n",
       "       [ 32,  32,  32, ..., 115, 116,  45],\n",
       "       ...,\n",
       "       [ 32,  32,  32, ..., 105, 110, 116],\n",
       "       [114, 101,  32, ...,  97, 110,  99],\n",
       "       [ 32, 118, 111, ..., 115, 117, 105]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bbb05dNynDrQ"
   },
   "source": [
    "### Build the model\n",
    "\n",
    "The model is defined as a two-layer, forward-LSTM, the same model should work both on CPU and TPU.\n",
    "\n",
    "Because our vocabulary size is 256, the input dimension to the Embedding layer is 256.\n",
    "\n",
    "When specifying the arguments to the LSTM, it is important to note how the stateful argument is used. When training we will make sure that `stateful=False` because we do want to reset the state of our model between batches, but when sampling (computing predictions) from a trained model, we want `stateful=True` so that the model can retain information across the current batch and generate more interesting text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yLEM-fLJlEEt"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 512\n",
    "\n",
    "def lstm_model(seq_len=100, batch_size=None, stateful=True):\n",
    "  \"\"\"Language model: predict the next word given the current word.\"\"\"\n",
    "  source = tf.keras.Input(\n",
    "      name='seed', shape=(seq_len,), batch_size=batch_size, dtype=tf.int32)\n",
    "\n",
    "  embedding = tf.keras.layers.Embedding(input_dim=256, output_dim=EMBEDDING_DIM)(source)\n",
    "  lstm_1 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(embedding)\n",
    "  lstm_2 = tf.keras.layers.LSTM(EMBEDDING_DIM, stateful=stateful, return_sequences=True)(lstm_1)\n",
    "  predicted_char = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(256, activation='softmax'))(lstm_2)\n",
    "  return tf.keras.Model(inputs=[source], outputs=[predicted_char])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzBYDJI0_Tfm"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "First, we need to create a distribution strategy that can use the TPU. In this case it is TPUStrategy. You can create and compile the model inside its scope. Once that is done, future calls to the standard Keras methods `fit`, `evaluate` and `predict` use the TPU.\n",
    "\n",
    "Again note that we train with `stateful=False` because while training, we only care about one batch at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the `TPU_NAME` env variable below so the TPUResolver can find your TPU instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TPU_NAME=auv-cloud-tpu\n"
     ]
    }
   ],
   "source": [
    "%env TPU_NAME=auv-cloud-tpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExQ922tfzSGA",
    "outputId": "c47f072d-8ce5-4128-a7df-539c3b6bdf91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: auv-cloud-tpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: auv-cloud-tpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU')]\n",
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "try:  # detect TPU\n",
    "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()  # resolver will search for the TPU_NAME env var\n",
    "    tf.config.experimental_connect_to_cluster(resolver)\n",
    "    # This is the TPU initialization code that has to be at the beginning.\n",
    "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "    strategy = tf.distribute.TPUStrategy(resolver)\n",
    "except ValueError:  # detect GPUs\n",
    "    strategy = tf.dictribute.MorroredStrategy()  # for GPU or multi-GPU machines\n",
    "\n",
    "with strategy.scope():\n",
    "    training_model = lstm_model(seq_len=100, stateful=False)\n",
    "    training_model.compile(\n",
    "        optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b7DRPdxDwY6o",
    "outputId": "b5542990-0ea4-4f41-fa19-a022a05e9e73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "seed (InputLayer)            [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 512)          131072    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100, 512)          2099200   \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 100, 256)          131328    \n",
      "=================================================================\n",
      "Total params: 4,460,800\n",
      "Trainable params: 4,460,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "training_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Nj_D2URwXoV",
    "outputId": "e55f033d-6462-4669-eac6-4065ec00a541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 13s 73ms/step - loss: 3.3996 - sparse_categorical_accuracy: 0.2031\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 3.1245 - sparse_categorical_accuracy: 0.2339\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 2.5325 - sparse_categorical_accuracy: 0.3137\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 7s 72ms/step - loss: 1.6893 - sparse_categorical_accuracy: 0.4894\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 1.2377 - sparse_categorical_accuracy: 0.6083\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.9842 - sparse_categorical_accuracy: 0.6802\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.7839 - sparse_categorical_accuracy: 0.7425\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.6098 - sparse_categorical_accuracy: 0.8004\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.4699 - sparse_categorical_accuracy: 0.8476\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.3700 - sparse_categorical_accuracy: 0.8815\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.2879 - sparse_categorical_accuracy: 0.9110\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 7s 73ms/step - loss: 0.2502 - sparse_categorical_accuracy: 0.9247\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.2369 - sparse_categorical_accuracy: 0.9292\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.2038 - sparse_categorical_accuracy: 0.9396\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.1834 - sparse_categorical_accuracy: 0.9454\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.1749 - sparse_categorical_accuracy: 0.9480\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.1934 - sparse_categorical_accuracy: 0.9421\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 7s 74ms/step - loss: 0.1678 - sparse_categorical_accuracy: 0.9497\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.1665 - sparse_categorical_accuracy: 0.9504\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 8s 75ms/step - loss: 0.1616 - sparse_categorical_accuracy: 0.9520\n"
     ]
    }
   ],
   "source": [
    "training_model.fit(\n",
    "    input_fn(),\n",
    "    steps_per_epoch=100,\n",
    "    epochs=20\n",
    ")\n",
    "training_model.save_weights('/tmp/fables.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCBtcpZkykSf"
   },
   "source": [
    "### Make predictions with the model\n",
    "\n",
    "Use the trained model to make predictions and generate your own Fontaine-esque play.\n",
    "Start the model off with a *seed* sentence, then generate 250 characters from it. The model makes five predictions from the initial seed.\n",
    "\n",
    "The predictions are done on the CPU so the batch size (5) in this case does not have to be divisible by 8.\n",
    "\n",
    "Note that when we are doing predictions or, to be more precise, text generation, we set `stateful=True` so that the model's state is kept between batches. If stateful is false, the model state is reset between each batch, and the model will only be able to use the information from the current batch (a single character) to make a prediction.\n",
    "\n",
    "The output of the model is a set of probabilities for the next character (given the input so far). To build a paragraph, we predict one character at a time and sample a character (based on the probabilities provided by the model). For example, if the input character is \"o\" and the output probabilities are \"p\" (0.65), \"t\" (0.30), others characters (0.05), then we allow our model to generate text other than just \"Ophelia\" and \"Othello.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tU7M-EGGxR3E",
    "outputId": "bd454ffc-8d94-4b1b-ecdb-05ed58ba489c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION 0\n",
      "\n",
      "\n",
      " il vous pourronver, se jouante!\n",
      "          Je le lendemain, en ma peine.\n",
      "  Ils alloient de partir, l'on peut donc pensée,\n",
      "  Écho retourner le gent bâton ne pouvoit faire,\n",
      "    Qu'on ne crut point à toute sorte,\n",
      "          Puis les a poissons y cor\n",
      "\n",
      "PREDICTION 1\n",
      "\n",
      "\n",
      " il vous faut point; Xantus par «requêtes livresée.\n",
      "  Ils étoient les corps certaines pareils de sommes,\n",
      "  Nous nous jours dépeut l'une est mon sentier, combien en cette office.\n",
      "  C'est beaucoup de bec. Que sert sa maison que donnent leur course s\n",
      "\n",
      "PREDICTION 2\n",
      "\n",
      "\n",
      " il vous sorte.\n",
      "  Allons presque plus servent en pieds.\n",
      "\n",
      "  [Illustration]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  [Illustration]\n",
      "\n",
      "  XIX\n",
      "\n",
      "  LE CHAREAU DES RAT.\n",
      "\n",
      "\n",
      "  On contributé, ressout mettre aussitôt ordu,\n",
      "          Mes prends son fort; mais les renvoie:\n",
      "          \n",
      "\n",
      "PREDICTION 3\n",
      "\n",
      "\n",
      " en meuteur jour est compagne.\n",
      "          On les fera pour le besoin\n",
      "          De son erreur en son cours;\n",
      "          Mais mon sens meuteusement\n",
      "          Qu'on soit mouches marié.\n",
      "          Après la main le tient point à la seine;\n",
      "  Et la pente \n",
      "\n",
      "PREDICTION 4\n",
      "\n",
      "\n",
      " ses jours avec quatre les nomes. Or trouver sa personne\n",
      "  L'un des bois déposieurs; et, Mort, autre seigneurient.\n",
      "  La dame de ses vents moins fourbes fort en pleurant;\n",
      "  Tu gens confils il est aussitôt! je serois rien.\n",
      "  De recourir aux rois vo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 5\n",
    "PREDICT_LEN = 250\n",
    "\n",
    "# Keras requires the batch size be specified ahead of time for stateful models.\n",
    "# We use a sequence length of 1, as we will be feeding in one character at a \n",
    "# time and predicting the next character.\n",
    "prediction_model = lstm_model(seq_len=1, batch_size=BATCH_SIZE, stateful=True)\n",
    "prediction_model.load_weights('/tmp/fables.h5')\n",
    "\n",
    "# We seed the model with our initial string, copied BATCH_SIZE times\n",
    "\n",
    "seed_txt = 'Maître corbeau, sur un arbre perché, '\n",
    "seed = transform(seed_txt)\n",
    "seed = np.repeat(np.expand_dims(seed, 0), BATCH_SIZE, axis=0)\n",
    "\n",
    "# First, run the seed forward to prime the state of the model.\n",
    "prediction_model.reset_states()\n",
    "for i in range(len(seed_txt) - 1):\n",
    "  prediction_model.predict(seed[:, i:i + 1])\n",
    "\n",
    "# Now we can accumulate predictions!\n",
    "predictions = [seed[:, -1:]]\n",
    "for i in range(PREDICT_LEN):\n",
    "  last_word = predictions[-1]\n",
    "  next_probits = prediction_model.predict(last_word)[:, 0, :]\n",
    "  \n",
    "  # sample from our output distribution\n",
    "  next_idx = [\n",
    "      np.random.choice(256, p=next_probits[i])\n",
    "      for i in range(BATCH_SIZE)\n",
    "  ]\n",
    "  predictions.append(np.asarray(next_idx, dtype=np.int32))\n",
    "  \n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "  print('PREDICTION %d\\n\\n' % i)\n",
    "  p = [predictions[j][i] for j in range(PREDICT_LEN)]\n",
    "  generated = ''.join([chr(c) for c in p])  # Convert back to text\n",
    "  print(generated)\n",
    "  print()\n",
    "  assert len(generated) == PREDICT_LEN, 'Generated text too short'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a5cGsSTEBQD"
   },
   "source": [
    "## What's next\n",
    "\n",
    "* Learn about [Cloud TPUs](https://cloud.google.com/tpu/docs) that Google designed and optimized specifically to speed up and scale up ML workloads for training and inference and to enable ML engineers and researchers to iterate more quickly.\n",
    "* Explore the range of [Cloud TPU tutorials and Colabs](https://cloud.google.com/tpu/docs/tutorials) to find other examples that can be used when implementing your ML project.\n",
    "\n",
    "On Google Cloud Platform, in addition to GPUs and TPUs available on pre-configured [deep learning VMs](https://cloud.google.com/deep-learning-vm/),  you will find [AutoML](https://cloud.google.com/automl/)*(beta)* for training custom models without writing code and [Cloud ML Engine](https://cloud.google.com/ml-engine/docs/) which will allows you to run parallel trainings and hyperparameter tuning of your custom models on powerful distributed hardware.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "N6ZDpd9XzFeN"
   ],
   "name": "Fables de La Fontaine with Cloud TPUs and Keras",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
